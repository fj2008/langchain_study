# 랭체인 스터디 4주차 (CH06 ~ CH15 까지)

## 공부 출처 https://wikidocs.net/book/14314

## 범위 6~15장

# Text Spliter

### 목적

LLM은 일반적으로 한 번 에 처리할 수 있는 토큰수에 제한이 있다. 너무 긴 텍스트는 잘려서 손실되거나 아예처리 되지 않을 수 있는데 이 긴 문서를 적절한 크기로 나눠 이 문제를 해결하기 위함

### 주요기능

1. 텍스트를 지정한 길이로 나눔
2. 오버랩 설정 가능 (문맥 유지를 위해 조각 사이에 일부내용을 겹치도록)
3. 문단, 문장, 단어 기준 나누기 등 다양한 기준 사용가능

예시

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,        # 각 조각의 최대 길이
    chunk_overlap=200       # 다음 조각과 겹치는 길이
)

chunks = text_splitter.split_text(long_text)
```

### 용도

보통 데이터 전처리를 하기위해 사용한다.

- PDF, 웹페이지, 책 등을 LLM에 넣기전에 전처리
- 벡터 DB에 넣어 검색용 임베딩을 만들기 전에
- QA 시스템에서 문서를 분할할때

**확실하진 않지만 GPT가 큰 문서를 넣으면 종종 컨텍스트를 까먹기도 하지만 이 긴 문서를 어떻게 효율적으로 인식할까 했는데 내부적으로 text spliter를 사용해서 한번 요약하는 과정을 거치는 것 같다. HTMLHeaderTextSpliter등을 보면 짐작가능..**

# Embedding

랭체인에서는 LLM모델과 상호작용하기위해 데이터를 임베딩 하기위한 여러 방식을 제공함

## 궁금증

#### Q. RAG 할때 사용하는 임베딩 모델은 꼭 베이스모델이 사용한 임베딩 모델이여야하나?

#### A. 상관 없음

- RAG파이프라인에 넣을 임베딩 모델은 반드시 베이스모델이 사용한 LLM모델과 같을 필요가 없다.
- 다만, 문서를 백터화할때 쓴 임베딩 모델과 나중에 사용자의 쿼리를 백터화할때 쓸 임베딩 모델은 반드시 동일해야한다.
- 왜 이게 가능할까?
  - 백터 검색시에는 임베딩된 벡터 공간을 사용하는데 이는 LLM과 분리돼있음 (백터db쓰니까)
  - 응답 생성시에는 자연어 토큰 시퀀스를 반환하여 LLM에서는 임베딩결과로 찾아온 텍스트만 필요하다.
  - 즉 검색에서는 문서와 쿼리가 같은 공간에만 있으면 되기때문에 상관없고
  - 생성단계에서 LLM은 백터를 보지 않고 검색기가 찾아낸 텍스트 프롬프트를 받아 이해하기때문에 상관없다.

#### Q. 위와 비슷한 맥락으로 파인튜닝시는 어떤지?

#### A. 무엇을 파인튜닝하나에 따라 다르다.

- 즉 파인튜닝 대상에 따라 달라짐

1. LLM만 파인튜닝할때 (거의 무관)

- 데이터 구성
  - (쿼리, 컨택스트, 정답) 형태로 LLM을 SET하면 RAG프롬프트 활용 능력이 향상됨
  - 이때 컨텍스트를 가져오는 임베딩 모델은 학슴 데이터 준비용 도구일뿐 lLM파라메터엔 관여안함
- 파인튜닝 교체 가능
  - 더 나은 임베딩 모델을 나중에 발견해도 문서 전체 재 임베딩만 하면 LLM은 그대로 사용가능

2. 임베딩 모델을 파인튜닝할 때 (메우 중요)

- 도메인 특화 효과
  - 의료 법률처럼 용어가 특수한 경우, 일반 임베딩은 유사도 순위가 깨진다.
  - 소량의 라벨(또는 LLM이 만든 synthetic 쌍)로 contrastive fine‑tuning 하면 R@k, nDCG가 눈에 띄게 오른다고한다.
- 주의할 점
  - 새 모델을 쓰면 차원-분포가 달라지므로 기존 인덱스는 무효, 문서- 쿼리를 모두 다시넣어야함
  - 쿼리 인퍼런스도 같은 파인튜닝된 모델로 해야한다.

3. 두 컴포넌트를 함께 파인튜닝 할 때 (상호의존 적)

- 엔드‑투‑엔드(E2E)
  - RA-DIT, RAFT, RankRAG 같은 기법은 *retriever loss*와 *generator loss*를 동시에 최소화합니다.
  - 초기 임베딩 모델이 좋을수록 빠르게 수렴하고, 나중에도 동일 구조(dual‑encoder + LLM)로 배포해야 합니다.
  - 복잡도 vs 성능
  - 성능은 가장 좋지만 학습 파이프라인, 리소스 요구가 크고, 롤백·AB테스트가 까다롭습니다.

# Vecter DB

임베딩한 데이터를 저장하는 DBMS. chroma를 매인으로 faiss, pinecone을 소개함

# Retriever

리트리버는 자연어 질문에 대해 관련 문서를 검색 하는 역할을 한다. LLM이 전체 문서를 모두 기억하거나 처리하지 않기때문에, 관련 정보만 추출해서 모델에게 전달하는 데에 핵심이 되는 구성요소다.

#### 구조

1. 문서 준비 -> 원문을 분할하고 임베딩
2. 백터 저장소에 저장
3. Retriever 사용 -> 질문을 입력하면, 유사한 문서를 가져옴
4. LLM에 전달 -> 가져온 문서를 바탕으로 답변 생성

예제

```python
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

# 1. 문서 로드 및 분할
loader = TextLoader("example.txt")
documents = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
split_docs = text_splitter.split_documents(documents)

# 2. 임베딩 및 벡터 저장소 생성
embedding = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(split_docs, embedding)

# 3. Retriever 생성
retriever = vectorstore.as_retriever()

# 4. QA 체인 생성
qa = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(),
    retriever=retriever,
    return_source_documents=True
)

# 5. 질문하기
query = "이 문서의 주요 내용은 뭐야?"
result = qa(query)
print(result['result'])  # 생성된 답변
```

# Reranker

Retriever와 함께 사용되며, 검색된 문서 중 정확도 높은 순서로 재정렬해주는 중요한 구성요소

- Retriever가 가져온 문서들 중에서 가장 관련있는 문서부터 정렬 해주는 구성요소
- 정확도 향상, 노이즈제거, 순서 조정
- Retriever가 여러 문서를 가져올 수 있기때문에, 그 중에서 가장 관련있는 문서를 우선순위로 정렬
- 특히 많은 문서를 검색한 후 최종 상위 N개를 고를 때 매우 유용
- Retriever의 정확도를 높이는 보조 장치격의 기능

### 사용도

- 문서가 적다 -> 필요없음
- 검색된 문서 수가 많다. -> 고려해볼만함
- 품질이 중요하다. -> 적극권장
- 비용과 속도에 민감하다 -> 신중히 고려
