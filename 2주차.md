# 랭체인 스터디 1주차 (~ CH01 까지)

## 공부 출처 https://wikidocs.net/book/14314

## 범위 4장

### 1장 다양한 LLM 모델 활용
#### openai gpt
모델 스펙 
링크: https://platform.openai.com/docs/models/gpt-4o
- model_name에 스펙에 나와있는 모델명을 선택할 수 있다
```python
# ChatOpenAI 객체를 생성합니다.
gpt = ChatOpenAI(
    temperature=0,
    model_name="gpt-4o",  # 모델명
)
```

#### Anthropic Claude
- API 키 발급: https://console.anthropic.com/settings/keys
- 모델 리스트: https://docs.anthropic.com/en/docs/about-claude/models
```python
from langchain_anthropic import ChatAnthropic

# ChatAnthropic 객체를 생성합니다.
anthropic = ChatAnthropic(model_name="claude-3-5-sonnet-20241022")

# 스트리밍 출력을 위하여 invoke() 대신 stream()을 사용합니다.
answer = anthropic.stream("사랑이 뭔가요?")

# 답변 출력
stream_response(answer)
```

한국 모델들도 있다. 

### 캐싱 전략
Lanchain은 LLM을 위한 선택적 캐싱 레이어를 제공합니다.
- 동일한 완료를 여러번 요청할때 API 호출횟수를 줄여 비용 절감
- LLM제공업체에 대한 API 호출횟수를 줄여 애플리케이션의 속도를 높일 수 있다.

#### InMemoryCache
- 메모리에 동일한 질문에 대한 답변을 저장하고 캐싱된 데이터를 반환
``` python
from langchain.globals import set_llm_cache
from langchain.cache import InMemoryCache

# 인메모리 캐시를 사용합니다.
set_llm_cache(InMemoryCache())

# 체인을 실행합니다.
response = chain.invoke({"country": "한국"})
print(response.content)
```
#### SQLite Cache
- DB에 동일한 질문의 답변을 저장
```python
from langchain_community.cache import SQLiteCache
from langchain_core.globals import set_llm_cache
import os

# 캐시 디렉토리를 생성합니다.
if not os.path.exists("cache"):
    os.makedirs("cache")

# SQLiteCache를 사용합니다.
set_llm_cache(SQLiteCache(database_path="cache/llm_cache.db"))
```


### 모델 직렬화
- 모델을 저장가능한 형식으로 변환하는 과정 (ai버전 git)
  - 모델 재사용
  - 모델 배포 및 공유하기 좋음
  - 계산 리소스 절약
  - 빠른 모델 로딩
  - 버전 관리 가능
  - 다양한 환경에서 사용가능

1. 직렬화 가능 여부 확인
```python
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

# 직렬화가 가능한지 체크합니다.
print(f"ChatOpenAI: {llm.is_lc_serializable()}")
```

### 체인 직렬화
- 직렬화 가능한 모든 객체를 딕셔너리 또는 Json문자열로 변환하는 과정
- 직렬화 방법
  - dumps: 객체를 JSON 문자열로 직렬화 
  - dumpd: 객체를 딕셔너리로 직렬화
  ```python
  from langchain_core.load import dumpd, dumps

  dumpd_chain = dumpd(chain)
  dumpd_chain
  ```
- 직렬화 모델 저장은 Picke로 하는 걸 추천하는 것 같다
- Pickle?
  - Pickle파일은 python객체를 바이너리형태로 직렬화 하는 포맷인데 아주 가볍고 복잡한 객체구조도 그대로 유지할 수 있으며 빠르게 직렬화하고 역직렬화 하는데 최적화 돼 있는 형식이다.
```python
import pickle

# fuit_chain.pkl 파일로 직렬화된 체인을 저장합니다.
with open("fruit_chain.pkl", "wb") as f:
    pickle.dump(dumpd_chain, f)

...
with open("fruit_chain.pkl", "rb") as f:
    loaded_chain = pickle.load(f)
# 불러오기
from langchain_core.load import load

# 체인을 로드합니다.
chain_from_file = load(loaded_chain)

# 체인을 실행합니다.
print(chain_from_file.invoke({"fruit": "사과"}))

```

### 토큰 사용량 확인
```
from langchain.callbacks import get_openai_callback
from langchain_openai import ChatOpenAI

# 모델을 불러옵니다.
llm = ChatOpenAI(model_name="gpt-4o")
# callback을 사용하여 추적합니다.
with get_openai_callback() as cb:
    result = llm.invoke("대한민국의 수도는 어디야?")
    print(cb)
... 
# output
Tokens Used: 55
    Prompt Tokens: 15
    Completion Tokens: 40
Successful Requests: 1
Total Cost (USD): $0.0006749999999999999
```

## LangChain의 Safety Settings 개념
LLM을 사용할때 출력 결과의 안정성, 필터링, 민감한 콘텐츠제어와 관련된 설정을 말한다. 특히 LLM제공업체의 API를 쓸 때 이 설정들이 중요하게 작용한다.
랭체인 자체는 LLM과 인터페이스를 중개하는 라이브러리라 LLM자체의 안정설정을 제어하거나 반영 할 수 있는 기능을 제공할 수 있다.

- OpenAI
  - openAI에서는 자체적으로 하기때문에 설정은 없음
- Claude
  - 명시적 지정 가능
  - 예시
    ```python
    from langchain.chat_models import ChatAnthropic
    chat = ChatAnthropic(
        anthropic_api_key="your-api-key",
        model="claude-2",
        safety_settings=[
            {
                "category": "HATE_SPEECH",
                "threshold": 2  # 0 = strict, 4 = lenient
            },
            {
                "category": "SEXUAL", # 이런거 내보내지 마!
                "threshold": 1
            }
        ]
    )
    ```

## 허깅페이스

### 허깅페이스 엔드포인트
- 허깅페이스?
  - 쉽게말해 AI 생태계에 github같은 플랫폼이다. 거기엔 자연어 처리(NLP), 이미지 생성, 번역 등 머신러닝 기능을 바로 사용할 수 있는 API(엔드포인트) 들이 있다. 즉 개발자가 복잡한 모델을 직접 학습시키지 않고도 필요하는 기능을 가져와서 ML앱을 만들 수 있다는 뜻
    - ML앱?
      - 사람이 직접 하지 않아도 되는 일을, 데이터를 학습한 모델이 자동으로 처리해주는 앱을 뜻함 (쳇봇, 이미지 분류앱, 번역기, 스팸필터, 추천 시스템 등등 즉 “머신러닝을 이용해서 뭔가 똑똑하게 자동으로 해주는 앱" 이라고 생각하면 좋음 langchain도 결국 ML앱을 쉽게 만들기 위한 도구라 봐도 무방하다.)


  - 모두 오픈 소스이며 공개적으로 사용 가능
- 허깅페이스 허브
  - 허깅페이스 허브는 다양한 ML애플리케이션 구축하기 위한 다양한 엔드포인트를 제공한다. 텍스트 생성 추론은 Text Generation Inference에 의해 구동됩니다. 이는 매우 빠른 텍스트 생성 추론을 위해 맞춤 제작된 Rust, Python, gRPC 서버입니다. (텍스트 추론은 겁나 빠르다는 뜻)

#### Inference Endpoints
Inference Endpoints는 머신러닝 모델을 실시간으로 서빙(서비스 제공)할 수 있는 기능이다. 쉽게 말해, 훈련된 모델을 API 형태로 배포해서 웹, 앱, 서버 등 어디서든 바로 사용할 수 있도록 해주는 서비스
- 특징
  - 서버리스
  - 실시간 API 제공
  - 보안은 API 키 인증을 통해 접근
  - 자동 스케일링 
    - 요청 수에 따라 자동으로 리소스를 조절함
  - 다양한 모델 지원
    - 멀티모달 텍스트모델 오디오 모델 등등 바로 배포 가능
- 참고
  - 무료 사용자는 일부 제약이 있다
  - 유료 요금제에서는 GPU서빙, 커스텀 도커 이미지 등 더 많은 기능이 제공
  - **모델을 꾸준히 배포해서 서비스로 운영하고 싶을때 유용하다**


### 허깅페이스 로컬
허깅페이스에 있는 모델을 다운로드 받아 로컬에서 사용하는 기능
HuggingFacePipeline 클래스를 통해 로컬에서 실행할 수 있도록 제공
```python
from langchain_huggingface import HuggingFacePipeline

llm = HuggingFacePipeline.from_model_id(
    model_id="microsoft/Phi-3-mini-4k-instruct",
    task="text-generation",
    pipeline_kwargs={
        "max_new_tokens": 256,
        "top_k": 50,
        "temperature": 0.1,
    },
)
llm.invoke("Hugging Face is")
```
### 허깅페이스 로컬 파이프라인
Hugging Face Model Hub는 많은 모델을 오픈 소스이고 공개적으로 사용 가능하여 사람들이 쉽게 협업하고 함께 ML을 구축할 수 있습니다.
transformers 가 있어야함
```
pip install transformers
```
#### transformers
Hugging Face Transformers 라이브러리의 pipeline은 복잡한 모델 사용을 쉽게 해주는 고수준 API이다
텍스트 생성, 번역, 감정 분석, 요약 등 다양한 작업을 몇 줄 코드로 실행할 수 있다.

- 기본 구조
  ```python
  from transformers import pipeline

  generator = pipeline("text-generation", model="gpt2")
  result = generator("Once upon a time", max_length=30)

  print(result)
  ```
  이렇게 하면 gpt2 모델을 Hugging Face에서 자동으로 다운로드하고 텍스트 생성을 실행

- 로컬 모델 파이프라인의 흐름
  1. Hugging Face Hub에서 모델을 다운로드
  2. 모델과 토크나이저를 로컬에 저장
  3. 로컬에서 pipeline()에 직접 로드해서 사용

#### 모델을 다운로드 및 저장
```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification

model_id = "distilbert-base-uncased-finetuned-sst-2-english"  # 감정 분석 모델

# 모델과 토크나이저 로드
model = AutoModelForSequenceClassification.from_pretrained(model_id)
tokenizer = AutoTokenizer.from_pretrained(model_id)

# 로컬 저장
model.save_pretrained("./my_model")
tokenizer.save_pretrained("./my_model")
```
### 로컬 모델로 파이프라인 사용
```python
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification

# 로컬 경로에서 로드
model_path = "./my_model"

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)

# 파이프라인 구성
clf = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)

# 테스트 실행
print(clf("I really love this product!"))
```
#### 파이프라인이 지원하는 태스크 예시
	- "sentiment-analysis" (감정 분석)
	- "text-classification" (텍스트 분류)
	-	"translation" (번역)
	-	"text-generation" (텍스트 생성)
	-	"summarization" (요약)
	-	"question-answering" (질문 응답)
	-	"image-classification" (이미지 분류) ← vision 모델 필요

### 그럼 로컬모델 파이프라인은 여러개 모델을 로드해서 종합적으로 사용하는 것인가?
Hugging Face의 pipeline()은 보통 하나의 모델을 하나의 태스크에 맞춰 사용하는 구조라 정답은 아니다 에 가깝다 
하지만, 여러개의 모델을 개별로 로드하고 각기 다른 용도로 사용한 뒤 결과를 종합적으로 처리하는 식으로 구성할 수 있다.
이를 모듈러 파이프라인 또는 커스텀 파이프라인 조합 이라고 한다.

