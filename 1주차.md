# 랭체인 스터디 1주차 (~ CH01 까지)

## 공부 출처 https://wikidocs.net/book/14314

## 범위 1~3장

## 내용

### 1장

- python 환경세팅 및 프로젝트 클론

### 2장

- openAI 에서 API 키 발급 받기

### 3장

- langchainSmith API키 발급 받기 및 환경 세팅

### 4장
- OpenAI API 사용
  - 객체 생성시 옵션값을 지정할 수 있다
    ```python
      # 객체 생성
      llm = ChatOpenAI(
          temperature=0.1,  # 창의성 (0.0 ~ 2.0)
          model_name="gpt-4o",  # 모델명
      )
    ```
    - 이 부분이 흥미롭다, 우리가 흔히 쓰는 chatGpt서비스는 이런값을 조정하지 않기때문 실제 서비스시에는 어떻게 동작할까?
      - GPT에게 직접 물어보니 UI로 제공되는 서비스에서는 옵션값들이 고정된 값으로 운영된다. 질문에따라서 자동으로 값을 수정하는 시스템은 없는듯하다... 
  
- LogProb
  - 주어진 텍스트에 대한 모델의 토큰 확률의 로그값을 의미, 문장을 구성하는 각 요소들을 의미하며 그 확률은 모델이 그 토큰을 예측할 확률을 나타낸다.
  - LogProb 계산 흐름
    1. 토큰 임베딩 -> 트렌스포머 블록 에서 고정된 크기의 임베딩 벡터로 변환된 뒤 여러층의 트렌스포머 블록을 거쳐 최종 hidden state "h"를 만든다.
    2. 언임베딩 과 logit계산
      - 마지막 h에 어휘 크기의 선형 투영을 적용해 각 토큰별 점수(logit)를 구한다.
    3. 구한 토큰별점수(로짓)에 소프트맥스를 적용해서 토큰별 확률"P"을 얻는다.
    4. 마지막으로 "P"에 로그를 씌워 변환하는 것이 logprob이다.
  - logProb을 활성화 한다는건 P값에 log를 한 값을 함께 돌려받겠다는 것이다.
  - **softmax기반 확률 분포에서 해당 토큰의 로그값이라 이해하면된다.**

- 멀티 모달 모델
  - 텍스만 있는게 아닌 여러가지 형태의 정보를 통합하여 처리하는 기술이나 접근방식을 의미한다.
  ```python
  from langchain_teddynote.models import MultiModal
  from langchain_teddynote.messages import stream_response

  # 객체 생성
  llm = ChatOpenAI(
      temperature=0.1,  # 창의성 (0.0 ~ 2.0)
      max_tokens=2048,  # 최대 토큰수
      model_name="gpt-4o",  # 모델명
  )

  # 멀티모달 객체 생성
  multimodal_llm = MultiModal(llm)
  # 샘플 이미지 주소(웹사이트로 부터 바로 인식)
  IMAGE_URL = "https://t3.ftcdn.net/jpg/03/77/33/96/360_F_377339633_Rtv9I77sSmSNcev8bEcnVxTHrXB4nRJ5.jpg"

  # 이미지 파일로 부터 질의
  answer = multimodal_llm.stream(IMAGE_URL)
  # 스트리밍 방식으로 각 토큰을 출력합니다. (실시간 출력)
  stream_response(answer)
  ```

- system, user프롬프트 수정
  - role을 부여하는 과정과 비슷하다
  ```python
  system_prompt = """당신은 표(재무제표) 를 해석하는 금융 AI 어시스턴트 입니다. 
  당신의 임무는 주어진 테이블 형식의 재무제표를 바탕으로 흥미로운 사실을 정리하여 친절하게 답변하는 것입니다."""

  user_prompt = """당신에게 주어진 표는 회사의 재무제표 입니다. 흥미로운 사실을 정리하여 답변하세요."""

  # 멀티모달 객체 생성
  multimodal_llm_with_prompt = MultiModal(
      llm, system_prompt=system_prompt, user_prompt=user_prompt
  )
  ```

### 5장
#### LangChain Experssion Language(LCEL)
  **LangChain Expression Language(줄여서 LCEL)**은 LangChain 구성 요소들을 선언형(declarative) 방식으로 ‘파이프(pipe, |)’ 연산자를 이용해 간단히 이어 붙여(chain) 실행할 수 있게 해주는 DSL(domain‑specific language)라고한다.
    
- 프롬프트 템플릿의 활용
  - 말그대로 프롬프트를 탬플릿화 해서 사용할 수 있는 기능
  ```python
  # template 정의
  template = "{country}의 수도는 어디인가요?"

  # from_template 메소드를 이용하여 PromptTemplate 객체 생성
  prompt_template = PromptTemplate.from_template(template)
  prompt_template
  # prompt 생성
  prompt = prompt_template.format(country="대한민국")
  prompt
  ```

#### chain 생성
다양한 구성 요소를 단일 체인으로 결합한 예제
```
chain = prompt | model | output_parser
```
파이프 연산자와 유사하고 서로 다른 구성요소를 연결하고 한 구성 요소의 출력을 다음 구성 요서의 입력으로 전달한다.
```python
# prompt 를 PromptTemplate 객체로 생성합니다.
prompt = PromptTemplate.from_template("{topic} 에 대해 쉽게 설명해주세요.")

model = ChatOpenAI()

chain = prompt | model
```

#### invoke() 호출
- 딕셔너리 형태로 입력값을 전달한다.
- invoke() 호출 시 입력값을 전달한다.
```python
chain = prompt | model
# input 딕셔너리에 주제를 '인공지능 모델의 학습 원리'으로 설정합니다.
input = {"topic": "인공지능 모델의 학습 원리"}
# prompt 객체와 model 객체를 파이프(|) 연산자로 연결하고 invoke 메서드를 사용하여 input을 전달합니다.
# 이를 통해 AI 모델이 생성한 메시지를 반환합니다.
chain.invoke(input)
```

#### 출력 파서 outputParser 적용
```python
chain = prompt | model
# chain 객체의 invoke 메서드를 사용하여 input을 전달합니다.
input = {"topic": "인공지능 모델의 학습 원리"}
chain.invoke(input)
```


### 6장
#### LCEL 인터페이스
LCEL에서 “인터페이스”라고 부르는 것은, LangChain Expression Language를 구성하는 러너블(Runnable) 객체들이 반드시 구현해야 하는 공통 메서드와 프로토콜을 가리킨다. 이 인터페이스 덕분에 서로 다른 처리 단위를 | 연산자로 깔끔하게 연결할 수 있다.

- Runnable 프로토콜
  - 사용자 정의 체인을 정의하고 표준방식으로 호출하는것을 쉽게 만든다.
    - 동기
      - stream: 응답의 청크를 스트리밍합니다.
      - invoke: 입력에 대해 체인을 호출합니다.
      - batch: 입력 목록에 대해 체인을 호출합니다.
    - 비동기 
      - astream: 비동기적으로 응답의 청크를 스트리밍합니다.
      - ainvoke: 비동기적으로 입력에 대해 체인을 호출합니다.
      - abatch: 비동기적으로 입력 목록에 대해 체인을 호출합니다.
      - astream_log: 최종 응답뿐만 아니라 발생하는 중간 단계를 스트리밍합니다.

### 7장
#### Runnable
LCEL의 기본단위이다. 체인으로 연결할 수 있는 모든 구성 요소 (언어 모델, 프롬프트 템플릿, 출력 파서, 검색기 등등)가 구현해야하는 공통 프로토콜, 이를 통해 다른 타입의 처리기를 일관되게 다룰수 있게된다.
- 통일성
- 확장성
- 검증 가능
#### 데이터를 효과적으로 전달하는 방법
- RunnablePassthrough
  ``` python
  prompt = PromptTemplate.from_template("{num} 의 10배는?")
  llm = ChatOpenAI(temperature=0)
  runnable_chain = {"num": RunnablePassthrough()} | prompt | ChatOpenAI()

  # dict 값이 RunnablePassthrough() 로 변경되었습니다.
  runnable_chain.invoke(10)
  ```
- RunnableParallel
  - Runnable 프로토콜을 병렬 실행에 특화해서 구현한 클래스
  ```python
  chain1 = (
    {"country": RunnablePassthrough()}
    | PromptTemplate.from_template("{country} 의 수도는?")
    | ChatOpenAI()
  )
  chain2 = (
      {"country": RunnablePassthrough()}
      | PromptTemplate.from_template("{country} 의 면적은?")
      | ChatOpenAI()
  )
  combined_chain = RunnableParallel(capital=chain1, area=chain2)
  combined_chain.invoke("대한민국")
  ### output
  {'capital': AIMessage(content='서울입니다.', response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 19, 'total_tokens': 24}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d9324c24-9670-4430-97d6-1272f5dbe0f2-0', usage_metadata={'input_tokens': 19, 'output_tokens': 5, 'total_tokens': 24}), 'area': AIMessage(content='대한민국의 총 면적은 약 100,363 km²입니다.', response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 20, 'total_tokens': 44}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-f27442a3-fc9c-4d08-9fdf-189c1b4585c8-0', usage_metadata={'input_tokens': 20, 'output_tokens': 24, 'total_tokens': 44})}
  ```
- RunnableLambda
  - 사용자 정의 함수를 맵핑할 수 있다.
  ```python
  def get_today(a):
    # 오늘 날짜를 가져오기
    return datetime.today().strftime("%b-%d")
    # prompt 와 llm 을 생성합니다.
  prompt = PromptTemplate.from_template(
      "{today} 가 생일인 유명인 {n} 명을 나열하세요. 생년월일을 표기해 주세요."
  )
  llm = ChatOpenAI(temperature=0, model_name="gpt-4o")

  # chain 을 생성합니다.
  chain = (
      {"today": RunnableLambda(get_today), "n": RunnablePassthrough()}
      | prompt
      | llm
      | StrOutputParser()
  )
  ```