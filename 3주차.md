# 랭체인 스터디 3주차 (~ CH01 까지)

## 공부 출처 https://wikidocs.net/book/14314

## 범위 5~6장

# Memory
기본적으로 어떠한 사전 정보를 미리 메모리에 띄워 놓고 이를 어떻게 활용하는가에 대한 방식들을 나열하는 장이다.
1. Memory 기능을 왜 사용하는가?

LangChain의 기본 설계는 “멀티턴 대화”나 “상태를 가진 작업”을 다루기 위한 것입니다.
그런데 일반 LLM (ex. GPT-4) 은 한번 요청할 때마다 과거 문맥을 기억하지 못합니다.
그래서 매번 과거 대화를 “다시” 입력해줘야 하는 문제가 있어요.

Memory는 이 문제를 해결하기 위해:
- 대화 내역을 기억하거나
- 사용자의 이전 입력,
- 모델의 이전 응답
등을 저장해서 다음 번 요청에 자동으로 붙여주는 기능입니다.

즉, “상태 유지” 를 위해 Memory가 필요합니다.

2. Memory를 사용하는 상황

다음과 같은 상황에서 Memory를 사용합니다:
	- 챗봇
→ 사용자와 여러 턴(turn)을 오가며 대화할 때, 앞서 한 말을 기억해야 자연스럽게 이어갈 수 있습니다.
	- Agent 시스템
→ 여러 번 행동을 하면서 중간 결과들을 기억하거나, 목표를 재설정할 때.
	- 사용자 맞춤형 서비스
→ 사용자 프로필, 과거 행동(예: 검색 기록, 선택지 등)을 기억하고 다음 답변에 반영할 때.
	- Q&A 시스템
→ 사용자가 한 시리즈의 질문을 이어서 할 때, 앞 질문의 맥락을 기억해야 할 때.

## 1-1 ConversationBufferMemory 
1. save_context 로 대화 내용을 기억하게 할 수 있음
2. 1에서 저장한대화내용을 llm에 memory로 넣어서 미리 출력할 대답을 어느정도 설정하게 할 수 있음
3. chain에 적용
  ```python
  from langchain_openai import ChatOpenAI
  from langchain.chains import ConversationChain

  # LLM 모델을 생성합니다.
  llm = ChatOpenAI(temperature=0, model_name="gpt-4o")

  # ConversationChain을 생성합니다.
  conversation = ConversationChain(
      # ConversationBufferMemory를 사용합니다.
      llm=llm,
      memory=ConversationBufferMemory(), # 여기서 save_context로 저장한 대화내용 memory에 띄운다.
  )
  # 대화를 시작합니다.
  response = conversation.predict(
      input="안녕하세요, 비대면으로 은행 계좌를 개설하고 싶습니다. 어떻게 시작해야 하나요?"
  )
  print(response)
  ```
## 1-2 ConversationBufferWindowMemory
1-1 과 비슷한 내용이나 다른점은 시간이 지남에 따라 대화의 상호작용 목록을 유지한다, 즉 모든 대화내용을 활용하는 것이 아닌 최근 K(설정가능)개의 상호작용만 사용한다. 이는 버퍼가 너무 커지는 것을 방지한다.

## 1-3 ConversationTokenBufferMemory
1-1 과 비슷한 내용, 다른점은 최근 대화의 히스토리를 버퍼를 메모리에 보관하고, 대화의 개수가 아닌 토큰 길이를 사용하여 대화내용을 flush할 시기를 결정한다.

- 예시
```python
from langchain.memory import ConversationTokenBufferMemory
from langchain_openai import ChatOpenAI


# LLM 모델 생성
llm = ChatOpenAI(model_name="gpt-4o")

# 메모리 설정
memory = ConversationTokenBufferMemory(
    llm=llm, max_token_limit=150, return_messages=True  # 최대 토큰 길이를 150개로 제한
)
```

## 1-4 ConversationEntityMemory
엔티티 메모리는 대화에서 특정 엔티티에 대한 주어진 사실을 기억한다.

## 1-5 ConversationKGMemory
그래프를 활용해서 서로 다른 개체간의 관계를 이해하는 데 도움을 주고 복잡한 연결망과 역사적 맥락을 기반으로 대응하는 능력을향상시킨다.


## 1-6 VectorStoreRetrieverMemory
save_context 를 할때 OpenAIEmbeddings() 를 활용해서 백터를 활용하는것,
대화내용을 백터로 기억하여 서빙하는것.

## 1-7 LCEl 대화내용 기억하기 (메모리 추가)
임의의 체인에 메모리를 추가하는 방법.

대화내용을 저장할 메모리인 `ConversationBufferMemory` 생성하고 `return_messages` 매개변수를 `True`로 설정하여, 생성된 인스턴스가 메시지를 반환하도록 합니다.
- `memory_key` 설정: 추후 Chain 의 `prompt` 안에 대입될 key 입니다. 변경하여 사용할 수 있습니다.


# 6장 DocumentLoder 
문서를 로드해서 프롬프트에서 활용하는 방식이다.
해당 장 에서는 문서의 종류가 많기 때문에 (excel, hwp, csv, word, powerpoint, 등등등) 각각의 문서별 로더를 소개하고 있다

##  DocumentLoader와 RAG(Retrieval-Augmented Generation)는 둘 다 “외부 정보를 가져와 활용하는 것”과 관련이 있어서 처음에는 비슷해 보일 수 있지만, 그 역할과 위치는 분명히 다르다!.

### DocumentLoder
목적 -> 텍스트 데이터를 가져와서 LangChain이 사용할 수 잇는 형식으로 변환하는 것.
#### 특징
- 데이터의 “입력 단계”를 담당.
- 다양한 소스로부터 데이터를 로드함.
- 예: PyPDFLoader, UnstructuredPDFLoader, WebBaseLoader, DirectoryLoader, 등
- 보통 이 결과물은 Document 객체의 리스트(List[Document])로 반환됨.
- 아직 이 단계에서는 벡터화(임베딩)나 검색, 생성이 없음.
- 예시
```python
from langchain.document_loaders import PyPDFLoader

loader = PyPDFLoader("example.pdf")
documents = loader.load()
```

### RAG (검색 기반 생성)
목적 -> 질문에 맞는 관련 문서를 검색하고, 그 문서를 기반으로 답변을 생성하는 전체적인 프레임 워크.

#### 특징
- 데이터 로딩 이후의 “활용 단계”.
- 주로 다음 구성요소 포함:
- Vector Store (임베딩된 문서 저장소)
- Retriever (질문과 유사한 문서를 찾아줌)
- LLM (문서 기반으로 응답 생성)
- 사용자의 쿼리를 벡터화하여 관련 문서를 검색(Retrieval)하고, 이 정보를 기반으로 LLM이 응답을 생성(Generation)함.
- 즉, RAG는 DocumentLoader의 출력물을 활용하는 전체 워크플로우임.
- 예시
```python
# 1. 문서 로딩
documents = loader.load()

# 2. 임베딩 및 벡터 저장
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings

db = FAISS.from_documents(documents, OpenAIEmbeddings())

# 3. 검색 및 생성
from langchain.chains import RetrievalQA
qa = RetrievalQA.from_chain_type(llm=llm, retriever=db.as_retriever())

# 4. 질문 입력
qa.run("문서에서 제품 보증 정책이 어떻게 되나요?")
```
- DocumentLoader는 데이터를 시스템 안으로 가져오는 “입구”
- RAG는 데이터를 검색하고 LLM이 답변하도록 하는 “전체 프로세스”

둘은 역할과 위치가 다르며, DocumentLoader는 종종 RAG의 준비 작업으로 쓰인다.

## 그럼 실시간 RAG업로드 기능을 서비스할 수 있을까?
- 현재 기술론 가능은 하나 신경써야할 포인트가 많다.

#### 문서 업로드 -> 실시간 임베딩 -> 벡터 스토어 추가
langChain에서는 다음과같은 기능을 제공한다
```python
# 1. 사용자가 업로드한 문서 처리
new_docs = loader.load()

# 2. 임베딩
from langchain.embeddings import OpenAIEmbeddings
embeddings = OpenAIEmbeddings()
new_vectors = embeddings.embed_documents([doc.page_content for doc in new_docs])

# 3. 기존 벡터스토어에 추가
db.add_documents(new_docs)
```

하지만 제약사항이 많다.
1. 임베딩 지연
    - 임베딩은 오래걸리는 작업임 실시간으로 했을시에 사용자 경험을 고려하면 여러 방법을 사용하여 사용자의 기다림을 햇지할 수 있어야한다.
2. LLM Context한계 
    - 검색된 문서가 많으면 LLM의 Context window를 넘길 수 있다.
3. 일부 vector DB는 완전힌 실시간 검색보다는 배치 기반에 더 적합하다.
4. 서버 재시작시 vectorStore를 유지하려면 물리적 저장이 필요한데 너무 무거움...

위와같은 제약을 해결하기위해 다양한 가라,,?! 훼이크 등을 사용할 수 있다. (캐싱, global vectorstroe 활용 등등) 하지만 고도화작업이 만만치 않을듯...

